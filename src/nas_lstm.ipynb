{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NAS - Optuna\n",
    "\n",
    "- **Authored by:** Matheus Ferreira Silva \n",
    "- **GitHub:**: https://github.com/MatheusFS-dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Async CUDA allocator\n",
    "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'\n",
    "\n",
    "# If cuDNN autotune fails, fall back to a safe (but slower) algorithm.\n",
    "os.environ[\"XLA_FLAGS\"] = \"--xla_gpu_strict_conv_algorithm_picker=false\"\n",
    "\n",
    "# Allow TensorFlow to allocate GPU memory as needed\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 08:45:31.285549: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-06 08:45:31.395712: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749210331.444205 2694486 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749210331.458508 2694486 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-06-06 08:45:31.562750: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/matheus/anaconda3/envs/tf-optuna/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from _imports import * # Centralized file containing all imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. GPU Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 08:45:33.970081: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "I0000 00:00:1749210333.970112 2694486 gpu_process_state.cc:201] Using CUDA malloc Async allocator for GPU: 0\n",
      "I0000 00:00:1749210333.971944 2694486 gpu_device.cc:2022] Created device /device:GPU:0 with 88 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070 Ti, pci bus id: 0000:b3:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.18.0\n",
      "CUDA support detected\n",
      "  CUDA Version: 12.5.1\n",
      "  cuDNN Version: 9\n",
      "\n",
      "GPUs Detected (1): ['/physical_device:GPU:0']\n",
      "Default GPU device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# Specify GPU to use (e.g., GPU 0)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Run Parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRIALS = 1\n",
    "EPOCHS = 50\n",
    "TOP_K = 5  # Number of top trials to save\n",
    "\n",
    "TOTAL_NUM_PORTS = 100\n",
    "observed_ports_list = [7]\n",
    "\n",
    "THRESHOLD = 1\n",
    "SNR_LINEAR = 2.5\n",
    "\n",
    "mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "\n",
    "#? Set to an existing path to resume training\n",
    "RESUME_TRAINING_PATH = \"runs/nas_lstm_v0\" # None or \"runs/nas_1\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run directory: runs/nas_lstm_v0\n"
     ]
    }
   ],
   "source": [
    "RUN_DIR = RESUME_TRAINING_PATH\n",
    "print(f\"Run directory: {RUN_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/w1_u1_n100/SNR_events_W1.0_U1_N100_kappa1.0e-16_mu1.0_m0.0.mat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/tf-optuna/lib/python3.11/site-packages/scipy/io/matlab/_mio.py:39\u001b[0m, in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_like\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# Probably \"not found\"\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/w1_u1_n100/SNR_events_W1.0_U1_N100_kappa1.0e-16_mu1.0_m0.0.mat'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# --------------------- Load the dataset in matlab format -------------------- #\u001b[39;00m\n\u001b[1;32m      2\u001b[0m rng \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mdefault_rng(\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m kappa0_mu1_m0 \u001b[38;5;241m=\u001b[39m \u001b[43mscipy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadmat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./data/w1_u1_n100/SNR_events_W1.0_U1_N100_kappa1.0e-16_mu1.0_m0.0.mat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSNR_events\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      5\u001b[0m kappa0_mu1_m2 \u001b[38;5;241m=\u001b[39m scipy\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mloadmat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data/w1_u1_n100/SNR_events_W1.0_U1_N100_kappa1.0e-16_mu1.0_m2.0.mat\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSNR_events\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      6\u001b[0m kappa0_mu1_m50 \u001b[38;5;241m=\u001b[39m scipy\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mloadmat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data/w1_u1_n100/SNR_events_W1.0_U1_N100_kappa1.0e-16_mu1.0_m50.0.mat\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSNR_events\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-optuna/lib/python3.11/site-packages/scipy/io/matlab/_mio.py:225\u001b[0m, in \u001b[0;36mloadmat\u001b[0;34m(file_name, mdict, appendmat, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;124;03mLoad MATLAB file.\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;124;03m    3.14159265+3.14159265j])\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    224\u001b[0m variable_names \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvariable_names\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 225\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_open_file_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mappendmat\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43mMR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmat_reader_factory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmatfile_dict\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mMR\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvariable_names\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-optuna/lib/python3.11/contextlib.py:137\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-optuna/lib/python3.11/site-packages/scipy/io/matlab/_mio.py:17\u001b[0m, in \u001b[0;36m_open_file_context\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;129m@contextmanager\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_context\u001b[39m(file_like, appendmat, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m---> 17\u001b[0m     f, opened \u001b[38;5;241m=\u001b[39m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_like\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mappendmat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m f\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-optuna/lib/python3.11/site-packages/scipy/io/matlab/_mio.py:45\u001b[0m, in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m appendmat \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m file_like\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.mat\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     44\u001b[0m         file_like \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.mat\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_like\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReader needs file name or open file-like object\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     49\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/w1_u1_n100/SNR_events_W1.0_U1_N100_kappa1.0e-16_mu1.0_m0.0.mat'"
     ]
    }
   ],
   "source": [
    "# --------------------- Load the dataset in matlab format -------------------- #\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "kappa0_mu1_m0 = scipy.io.loadmat(\"./data/w1_u1_n100/SNR_events_W1.0_U1_N100_kappa1.0e-16_mu1.0_m0.0.mat\")[\"SNR_events\"]\n",
    "kappa0_mu1_m2 = scipy.io.loadmat(\"./data/w1_u1_n100/SNR_events_W1.0_U1_N100_kappa1.0e-16_mu1.0_m2.0.mat\")[\"SNR_events\"]\n",
    "kappa0_mu1_m50 = scipy.io.loadmat(\"./data/w1_u1_n100/SNR_events_W1.0_U1_N100_kappa1.0e-16_mu1.0_m50.0.mat\")[\"SNR_events\"]\n",
    "kappa0_mu2_m50 = scipy.io.loadmat(\"./data/w1_u1_n100/SNR_events_W1.0_U1_N100_kappa1.0e-16_mu2.0_m50.0.mat\")[\"SNR_events\"]\n",
    "kappa0_mu5_m50 = scipy.io.loadmat(\"./data/w1_u1_n100/SNR_events_W1.0_U1_N100_kappa1.0e-16_mu5.0_m50.0.mat\")[\"SNR_events\"]\n",
    "kappa5_mu1_m0 = scipy.io.loadmat(\"./data/w1_u1_n100/SNR_events_W1.0_U1_N100_kappa5.0e+00_mu1.0_m0.0.mat\")[\"SNR_events\"]\n",
    "kappa5_mu1_m2 = scipy.io.loadmat(\"./data/w1_u1_n100/SNR_events_W1.0_U1_N100_kappa5.0e+00_mu1.0_m2.0.mat\")[\"SNR_events\"]\n",
    "kappa5_mu1_m50 = scipy.io.loadmat(\"./data/w1_u1_n100/SNR_events_W1.0_U1_N100_kappa5.0e+00_mu1.0_m50.0.mat\")[\"SNR_events\"]\n",
    "kappa5_mu2_m0 = scipy.io.loadmat(\"./data/w1_u1_n100/SNR_events_W1.0_U1_N100_kappa5.0e+00_mu2.0_m0.0.mat\")[\"SNR_events\"]\n",
    "kappa5_mu2_m2 = scipy.io.loadmat(\"./data/w1_u1_n100/SNR_events_W1.0_U1_N100_kappa5.0e+00_mu2.0_m2.0.mat\")[\"SNR_events\"]\n",
    "kappa5_mu2_m50 = scipy.io.loadmat(\"./data/w1_u1_n100/SNR_events_W1.0_U1_N100_kappa5.0e+00_mu2.0_m50.0.mat\")[\"SNR_events\"]\n",
    "kappa5_mu5_m0 = scipy.io.loadmat(\"./data/w1_u1_n100/SNR_events_W1.0_U1_N100_kappa5.0e+00_mu5.0_m0.0.mat\")[\"SNR_events\"]\n",
    "kappa5_mu5_m2 = scipy.io.loadmat(\"./data/w1_u1_n100/SNR_events_W1.0_U1_N100_kappa5.0e+00_mu5.0_m2.0.mat\")[\"SNR_events\"]\n",
    "kappa5_mu5_m50 = scipy.io.loadmat(\"./data/w1_u1_n100/SNR_events_W1.0_U1_N100_kappa5.0e+00_mu5.0_m50.0.mat\")[\"SNR_events\"]\n",
    "\n",
    "# ————————————— Split the data into 10% training and 90% testing ————————————— #\n",
    "\n",
    "# kappa0_mu1_m0\n",
    "perm = rng.permutation(kappa0_mu1_m0.shape[0])\n",
    "n_test = int(0.9*kappa0_mu1_m0.shape[0])\n",
    "kappa0_mu1_m0_test = kappa0_mu1_m0[perm[:n_test]]\n",
    "kappa0_mu1_m0 = kappa0_mu1_m0[perm[n_test:]]\n",
    "\n",
    "# kappa0_mu1_m2\n",
    "perm = rng.permutation(kappa0_mu1_m2.shape[0])\n",
    "n_test = int(0.9*kappa0_mu1_m2.shape[0])\n",
    "kappa0_mu1_m2_test = kappa0_mu1_m2[perm[:n_test]]\n",
    "kappa0_mu1_m2 = kappa0_mu1_m2[perm[n_test:]]\n",
    "\n",
    "# kappa0_mu1_m50\n",
    "perm = rng.permutation(kappa0_mu1_m50.shape[0])\n",
    "n_test = int(0.9*kappa0_mu1_m50.shape[0])\n",
    "kappa0_mu1_m50_test = kappa0_mu1_m50[perm[:n_test]]\n",
    "kappa0_mu1_m50 = kappa0_mu1_m50[perm[n_test:]]\n",
    "\n",
    "# kappa0_mu2_m50\n",
    "perm = rng.permutation(kappa0_mu2_m50.shape[0])\n",
    "n_test = int(0.9*kappa0_mu2_m50.shape[0])\n",
    "kappa0_mu2_m50_test = kappa0_mu2_m50[perm[:n_test]]\n",
    "kappa0_mu2_m50 = kappa0_mu2_m50[perm[n_test:]]\n",
    "\n",
    "# kappa0_mu5_m50\n",
    "perm = rng.permutation(kappa0_mu5_m50.shape[0])\n",
    "n_test = int(0.9*kappa0_mu5_m50.shape[0])\n",
    "kappa0_mu5_m50_test = kappa0_mu5_m50[perm[:n_test]]\n",
    "kappa0_mu5_m50 = kappa0_mu5_m50[perm[n_test:]]\n",
    "\n",
    "# kappa5_mu1_m0\n",
    "perm = rng.permutation(kappa5_mu1_m0.shape[0])\n",
    "n_test = int(0.9*kappa5_mu1_m0.shape[0])\n",
    "kappa5_mu1_m0_test = kappa5_mu1_m0[perm[:n_test]]\n",
    "kappa5_mu1_m0 = kappa5_mu1_m0[perm[n_test:]]\n",
    "\n",
    "# kappa5_mu1_m2\n",
    "perm = rng.permutation(kappa5_mu1_m2.shape[0])\n",
    "n_test = int(0.9*kappa5_mu1_m2.shape[0])\n",
    "kappa5_mu1_m2_test = kappa5_mu1_m2[perm[:n_test]]\n",
    "kappa5_mu1_m2 = kappa5_mu1_m2[perm[n_test:]]\n",
    "\n",
    "# kappa5_mu1_m50\n",
    "perm = rng.permutation(kappa5_mu1_m50.shape[0])\n",
    "n_test = int(0.9*kappa5_mu1_m50.shape[0])\n",
    "kappa5_mu1_m50_test = kappa5_mu1_m50[perm[:n_test]]\n",
    "kappa5_mu1_m50 = kappa5_mu1_m50[perm[n_test:]]\n",
    "\n",
    "# kappa5_mu2_m0\n",
    "perm = rng.permutation(kappa5_mu2_m0.shape[0])\n",
    "n_test = int(0.9*kappa5_mu2_m0.shape[0])\n",
    "kappa5_mu2_m0_test = kappa5_mu2_m0[perm[:n_test]]\n",
    "kappa5_mu2_m0 = kappa5_mu2_m0[perm[n_test:]]\n",
    "\n",
    "# kappa5_mu2_m2\n",
    "perm = rng.permutation(kappa5_mu2_m2.shape[0])\n",
    "n_test = int(0.9*kappa5_mu2_m2.shape[0])\n",
    "kappa5_mu2_m2_test = kappa5_mu2_m2[perm[:n_test]]\n",
    "kappa5_mu2_m2 = kappa5_mu2_m2[perm[n_test:]]\n",
    "\n",
    "# kappa5_mu2_m50\n",
    "perm = rng.permutation(kappa5_mu2_m50.shape[0])\n",
    "n_test = int(0.9*kappa5_mu2_m50.shape[0])\n",
    "kappa5_mu2_m50_test = kappa5_mu2_m50[perm[:n_test]]\n",
    "kappa5_mu2_m50 = kappa5_mu2_m50[perm[n_test:]]\n",
    "\n",
    "# kappa5_mu5_m0\n",
    "perm = rng.permutation(kappa5_mu5_m0.shape[0])\n",
    "n_test = int(0.9*kappa5_mu5_m0.shape[0])\n",
    "kappa5_mu5_m0_test = kappa5_mu5_m0[perm[:n_test]]\n",
    "kappa5_mu5_m0 = kappa5_mu5_m0[perm[n_test:]]\n",
    "\n",
    "# kappa5_mu5_m2\n",
    "perm = rng.permutation(kappa5_mu5_m2.shape[0])\n",
    "n_test = int(0.9*kappa5_mu5_m2.shape[0])\n",
    "kappa5_mu5_m2_test = kappa5_mu5_m2[perm[:n_test]]\n",
    "kappa5_mu5_m2 = kappa5_mu5_m2[perm[n_test:]]\n",
    "\n",
    "# kappa5_mu5_m50\n",
    "perm = rng.permutation(kappa5_mu5_m50.shape[0])\n",
    "n_test = int(0.9*kappa5_mu5_m50.shape[0])\n",
    "kappa5_mu5_m50_test = kappa5_mu5_m50[perm[:n_test]]\n",
    "kappa5_mu5_m50 = kappa5_mu5_m50[perm[n_test:]]\n",
    "\n",
    "# ————————————— Concatenate all training subsamples along axis=0 ————————————— #\n",
    "dataset = np.concatenate(\n",
    "    [\n",
    "        kappa0_mu1_m0,\n",
    "        kappa0_mu1_m2,\n",
    "        kappa0_mu1_m50,\n",
    "        kappa0_mu2_m50,\n",
    "        kappa0_mu5_m50,\n",
    "        kappa5_mu1_m0,\n",
    "        kappa5_mu1_m2,\n",
    "        kappa5_mu1_m50,\n",
    "        kappa5_mu2_m0,\n",
    "        kappa5_mu2_m2,\n",
    "        kappa5_mu2_m50,\n",
    "        kappa5_mu5_m0,\n",
    "        kappa5_mu5_m2,\n",
    "        kappa5_mu5_m50,\n",
    "    ],\n",
    "    axis=0,\n",
    ")\n",
    "\n",
    "print(f\"Original dataset shape: {dataset.shape}\")\n",
    "\n",
    "# Subsample data\n",
    "# dataset = dataset[: int(0.01 * dataset.shape[0]), :]\n",
    "\n",
    "print(f\"Shape of the data after configuration: {dataset.shape}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Getters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regularizer(trial: optuna.Trial, name: str) -> Optional[tf.keras.regularizers.Regularizer]:\n",
    "    \"\"\"\n",
    "    Suggests a regularization strategy using Optuna and returns the corresponding Keras regularizer.\n",
    "    \n",
    "    Args:\n",
    "        trial (optuna.Trial): Optuna trial object used to sample the regularizer.\n",
    "        name (str): Unique identifier for this regularizer parameter (used as key).\n",
    "\n",
    "    Returns:\n",
    "        Optional[tf.keras.regularizers.Regularizer]: The selected Keras regularizer instance,\n",
    "        or `None` if \"none\" was selected.\n",
    "    \"\"\"\n",
    "    # Suggest a regularizer type\n",
    "    reg_type: str = trial.suggest_categorical(\n",
    "        name,\n",
    "        [\n",
    "            \"none\",\n",
    "            \"l1\",\n",
    "            \"l2\",\n",
    "            \"l1l2\",\n",
    "            # \"orthogonal\",  #! only works for rank-2 tensors\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Map each regularizer name to a corresponding Keras regularizer instance\n",
    "    regularizer_map: Dict[str, Optional[tf.keras.regularizers.Regularizer]] = {\n",
    "        \"none\": None,\n",
    "        \"l1\": regularizers.L1(l1=0.01),\n",
    "        \"l2\": regularizers.L2(l2=0.01),\n",
    "        \"l1l2\": regularizers.L1L2(l1=0.01, l2=0.01),\n",
    "        \"orthogonal\": regularizers.OrthogonalRegularizer(factor=0.01, mode=\"rows\"),\n",
    "    }\n",
    "\n",
    "    # Return the appropriate regularizer, or None if not found\n",
    "    return regularizer_map.get(reg_type, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation(trial: Any, name: str) -> Union[str, Callable[..., layers.Layer]]:\n",
    "    \"\"\"\n",
    "    Suggests an activation function from a predefined list using Optuna.\n",
    "\n",
    "    Args:\n",
    "        trial (Any): The Optuna trial instance used to suggest a value.\n",
    "        name (str): A unique name for this hyperparameter (e.g., \"layer_1_activation\").\n",
    "\n",
    "    Returns:\n",
    "        Union[str, Callable[..., layers.Layer]]: A string representing the activation function.\n",
    "        This can be passed directly into a Keras layer's `activation=` argument.\n",
    "    \"\"\"\n",
    "    return trial.suggest_categorical(\n",
    "        name,\n",
    "        [\n",
    "            \"relu\",\n",
    "            \"tanh\",\n",
    "            \"sigmoid\",  # Logistic\n",
    "            \"elu\", \n",
    "            \"swish\",  # x * sigmoid(x)\n",
    "            \"leaky_relu\",\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(trial: optuna.Trial) -> tf.keras.optimizers.Optimizer:\n",
    "    \"\"\"\n",
    "    Suggests and returns a TensorFlow optimizer with a trial-based learning rate.\n",
    "\n",
    "    Args:\n",
    "        trial (optuna.Trial): Optuna trial object used for hyperparameter suggestion.\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.optimizers.Optimizer: An instance of the selected optimizer.\n",
    "    \"\"\"\n",
    "    # Suggest optimizer name from a predefined categorical set\n",
    "    optimizer_name = trial.suggest_categorical(\n",
    "        \"optimizer\",\n",
    "        [\n",
    "            \"AdamW\",\n",
    "            # \"SGD\",\n",
    "            # \"Adam\",\n",
    "            # \"RMSprop\",\n",
    "            # \"Nadam\",\n",
    "            # \"Lion\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Suggest learning rate on a logarithmic scale between 1e-5 and 1e-2\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-2, log=True)\n",
    "\n",
    "    # Mapping of optimizer names to their TensorFlow classes\n",
    "    optimizer_map: Dict[str, Type[tf.keras.optimizers.Optimizer]] = {\n",
    "        \"Adam\": optimizers.Adam,\n",
    "        \"AdamW\": optimizers.AdamW,\n",
    "        \"SGD\": optimizers.SGD,\n",
    "        \"RMSprop\": optimizers.RMSprop,\n",
    "        \"Nadam\": optimizers.Nadam,\n",
    "        \"Lion\": optimizers.Lion,\n",
    "    }\n",
    "\n",
    "    # Raise error if selected optimizer is not supported in the current context\n",
    "    if optimizer_name not in optimizer_map:\n",
    "        raise ValueError(\n",
    "            f\"Optimizer '{optimizer_name}' is not supported. \"\n",
    "            f\"Supported optimizers are: {list(optimizer_map.keys())}.\"\n",
    "        )\n",
    "\n",
    "    # Instantiate and return the selected optimizer with suggested learning rate\n",
    "    return optimizer_map[optimizer_name](learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callbacks(trial: optuna.Trial, checkpoint_dir: str) -> List[tf.keras.callbacks.Callback]:\n",
    "    \"\"\"\n",
    "    Constructs and returns a list of Keras callbacks tailored for Optuna trials.\n",
    "\n",
    "    Args:\n",
    "        trial (optuna.Trial): The current Optuna trial object.\n",
    "        checkpoint_dir (str): Directory where model weights will be saved.\n",
    "\n",
    "    Returns:\n",
    "        List[tf.keras.callbacks.Callback]: A list of callbacks to pass into `model.fit()`.\n",
    "    \"\"\"\n",
    "    # Construct path for saving weights for this specific trial\n",
    "    checkpoint_path: str = os.path.join(checkpoint_dir, f\"trial_{trial.number}.weights.h5\")\n",
    "\n",
    "    # Metric to monitor for early stopping and checkpointing\n",
    "    monitor: str = \"val_loss\"\n",
    "\n",
    "    # Stop training early if no improvement in validation loss for N epochs\n",
    "    early_stopping = callbacks.EarlyStopping(\n",
    "        monitor=monitor,\n",
    "        patience=6,  # number of epochs to wait\n",
    "        restore_best_weights=True,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    # Reduce learning rate if validation loss plateaus\n",
    "    reduce_lr = callbacks.ReduceLROnPlateau(\n",
    "        monitor=monitor,\n",
    "        patience=3,  # how many epochs to wait before reducing LR\n",
    "        factor=0.2,  # reduce LR by this factor\n",
    "        min_lr=1e-6,  # don't reduce below this\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    # Save only the best model weights based on monitored metric\n",
    "    model_checkpoint = callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_path,\n",
    "        monitor=monitor,\n",
    "        save_best_only=True,  # only save weights if val_loss improves\n",
    "        save_weights_only=True,  # save only the weights (not full model)\n",
    "        verbose=0,\n",
    "    )\n",
    "\n",
    "    #! ——————— WARNING: the callbacks below do not work with multi-objective —————— !#\n",
    "    # Custom callback to prune trial if NaN loss is encountered\n",
    "    nan_pruner_callback = NanLossPrunerCallback(trial)\n",
    "\n",
    "    # Optuna's built-in pruning callback for early trial termination\n",
    "    pruning_callback = KerasPruningCallback(trial, monitor)\n",
    "    #! ———————————————————————————————————————————————————————————————————————————— !#\n",
    "\n",
    "    # Return the complete list of callbacks\n",
    "    return [early_stopping, reduce_lr, model_checkpoint, nan_pruner_callback, pruning_callback]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5. Scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scaler(\n",
    "    trial: optuna.Trial,\n",
    ") -> Union[StandardScaler, MinMaxScaler, RobustScaler, QuantileTransformer, PowerTransformer]:\n",
    "    \"\"\"\n",
    "    Suggests and returns a scikit-learn scaler based on Optuna hyperparameter selection.\n",
    "\n",
    "    Args:\n",
    "        trial (optuna.Trial): Optuna trial object used to suggest hyperparameters.\n",
    "\n",
    "    Returns:\n",
    "        Union[StandardScaler, MinMaxScaler, RobustScaler, QuantileTransformer, PowerTransformer]:\n",
    "            Instantiated scaler object from scikit-learn.\n",
    "    \"\"\"\n",
    "    # Suggest a scaler name from the list of supported options\n",
    "    scaler_name = trial.suggest_categorical(\n",
    "        \"scaler\",\n",
    "        [\n",
    "            \"StandardScaler\",  # For normally-distributed data\n",
    "            \"MinMaxScaler_-1_1\",  # Normalize to [-1, 1] range\n",
    "            \"MinMaxScaler_0_1\",  # Normalize to [0, 1] range\n",
    "            # \"RobustScaler\",  # For data with outliers\n",
    "            # \"QuantileTransformer\",  # For non-normal or skewed data\n",
    "            # \"PowerTransformer\",  # For heavy-tailed or skewed data\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Return the appropriate scaler instance based on selection\n",
    "    if scaler_name == \"StandardScaler\":\n",
    "        return StandardScaler()\n",
    "    elif scaler_name == \"RobustScaler\":\n",
    "        return RobustScaler()\n",
    "    elif scaler_name == \"QuantileTransformer\":\n",
    "        return QuantileTransformer(output_distribution=\"normal\")\n",
    "    elif scaler_name == \"PowerTransformer\":\n",
    "        return PowerTransformer(method=\"yeo-johnson\")\n",
    "    elif scaler_name == \"MinMaxScaler_0_1\":\n",
    "        return MinMaxScaler(feature_range=(0, 1))\n",
    "    elif scaler_name == \"MinMaxScaler_-1_1\":\n",
    "        return MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "    # Catch invalid or unknown choices\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown scaler selected: {scaler_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6. Implementation getters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_observed_ports(sinr_data, num_observed_ports, total_ports):\n",
    "    \"\"\"\n",
    "    Extracts SINR values for the specified number of observed ports.\n",
    "\n",
    "    The function selects a subset of SINR data by identifying equally spaced ports based on the\n",
    "    number of observed ports specified. It returns the SINR values for these observed ports and\n",
    "    their corresponding indices.\n",
    "\n",
    "    Args:\n",
    "        sinr_data (numpy.ndarray): A 2D array where each row represents an observation and each column\n",
    "                                   represents a port with its corresponding SINR values.\n",
    "        num_observed_ports (int): The number of observed ports to select from the SINR data.\n",
    "        total_ports (int): The total number of ports in the SINR data.\n",
    "\n",
    "    Returns:\n",
    "        observed_sinr (numpy.ndarray): A 2D array containing the SINR values for the observed ports.\n",
    "        observed_indices (numpy.ndarray): A 1D array of the indices corresponding to the observed ports.\n",
    "    \"\"\"\n",
    "    observed_indices = np.linspace(0, total_ports - 1, num_observed_ports, dtype=int)\n",
    "    observed_sinr = sinr_data[:, observed_indices]\n",
    "\n",
    "    print(f\"Observed indices for {num_observed_ports} ports: {observed_indices}\")\n",
    "\n",
    "    return observed_sinr, observed_indices\n",
    "\n",
    "\n",
    "def getOP(\n",
    "    observed_indices: np.ndarray, \n",
    "    predicted_values: np.ndarray, \n",
    "    true_values: np.ndarray, \n",
    "    threshold: float, \n",
    "    snr_linear: float,\n",
    "    total_ports: int\n",
    ") -> float:\n",
    "    \"\"\"Estimate the outage probability for regression models.\n",
    "\n",
    "    This function compares the predicted and observed signal values at different \n",
    "    channels (ports) and determines whether the chosen signal is above a given threshold. \n",
    "    The outage probability is then computed as the proportion of times the signal falls \n",
    "    below this threshold.\n",
    "\n",
    "    Args:\n",
    "        observed_indices (np.ndarray): Indices of the observed ports (channels).\n",
    "        predicted_values (np.ndarray): Matrix of predicted values for each sample.\n",
    "        true_values (np.ndarray): Ground-truth values for each port.\n",
    "        threshold (float): Threshold value for determining outage.\n",
    "        snr_linear (float): Signal-to-noise ratio in linear scale.\n",
    "\n",
    "    Returns:\n",
    "        float: Estimated outage probability.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize an array with negative infinity to store the observed values\n",
    "    observed_values_matrix = np.full((true_values.shape[0], total_ports), -np.inf, dtype=np.float64)\n",
    "\n",
    "    # Assign the true values of the observed ports (channels) to the matrix\n",
    "    observed_values_matrix[:, observed_indices] = true_values[:, observed_indices]\n",
    "\n",
    "    # Find the index of the highest predicted value for each sample\n",
    "    best_predicted_indices = np.argmax(predicted_values, axis=1)\n",
    "\n",
    "    # Initialize an array with negative infinity to store the predicted values\n",
    "    predicted_values_matrix = np.full((true_values.shape[0], total_ports), -np.inf, dtype=np.float64)\n",
    "\n",
    "    # Assign the true value corresponding to the predicted best port\n",
    "    predicted_values_matrix[np.arange(len(best_predicted_indices)), best_predicted_indices] = (\n",
    "        true_values[np.arange(len(best_predicted_indices)), best_predicted_indices]\n",
    "    )\n",
    "\n",
    "    # Take the element-wise maximum between the observed and predicted value matrices\n",
    "    best_value_matrix = np.maximum(observed_values_matrix, predicted_values_matrix)\n",
    "\n",
    "    # print(\"Shape of Best Value Matrix:\", best_value_matrix.shape)\n",
    "\n",
    "    # Find the index of the best predicted or observed port (channel) for each sample\n",
    "    best_predicted_or_observed_ports = np.argmax(best_value_matrix, axis=1)\n",
    "\n",
    "    # print(\"Shape of Best Predicted/Observed Ports:\", best_predicted_or_observed_ports.shape)\n",
    "    # print(\"Number of Selected Ports:\", len(best_predicted_or_observed_ports))\n",
    "\n",
    "    # Retrieve the actual values corresponding to the best selected ports\n",
    "    selected_values = best_value_matrix[np.arange(len(true_values)), best_predicted_or_observed_ports]\n",
    "\n",
    "    # print(\"Shape of Selected Values:\", selected_values.shape)\n",
    "\n",
    "    # Determine which selected values are above the given threshold\n",
    "    above_threshold = selected_values > (threshold / snr_linear)\n",
    "\n",
    "    # print(\"Shape of Above Threshold Array:\", above_threshold.shape)\n",
    "\n",
    "    # Compute the outage probability: probability that the selected value is below the threshold\n",
    "    outage_probability = 1.0 - (np.sum(above_threshold) / len(true_values))\n",
    "\n",
    "    return outage_probability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Layers Builders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dynamic_lstm(\n",
    "    trial: optuna.Trial,\n",
    "    x: layers.Layer,\n",
    "    max_layers: int = 5,\n",
    "    max_units: int = 256,\n",
    "    min_units: int = 32,\n",
    "    unit_step: int = 32,\n",
    "    try_batch_norm: bool = False,\n",
    "    use_regularization: bool = False,\n",
    "    residual_method: Optional[str] = None,\n",
    "    custom_name: str = \"lstm\",\n",
    ") -> layers.Layer:\n",
    "    \"\"\"\n",
    "    Build a stacked 1D LSTM with optional BatchNormalization and residual connections.\n",
    "\n",
    "    Args:\n",
    "        trial (optuna.Trial): Trial for hyperparameter suggestions.\n",
    "        x (Layer): Input tensor.\n",
    "        max_layers (int): Maximum number of LSTM blocks. Defaults to 5.\n",
    "        max_units (int): Maximum number of units for the first block. Defaults to 256.\n",
    "        min_units (int): Minimum number of units for the first block. Defaults to 32.\n",
    "        unit_step (int): Step size when sampling the number of units. Defaults to 32.\n",
    "        try_batch_norm (bool): Whether to allow BatchNormalization after each LSTM. Defaults to False.\n",
    "        use_regularization (bool): Whether to apply kernel/bias/activity regularizers. Defaults to False.\n",
    "        residual_method (Optional[str]): One of {\"beside\", \"all\", None} for skip connections. Defaults to None.\n",
    "        custom_name (str): Prefix for naming layers. Defaults to \"lstm\".\n",
    "\n",
    "    Returns:\n",
    "        Layer: Tensor after successive LSTM → (BatchNorm → Residual) blocks.\n",
    "    \"\"\"\n",
    "    residual_tensor = None\n",
    "    skip_tensors = []\n",
    "\n",
    "    for i in range(max_layers):\n",
    "        units = trial.suggest_int(\n",
    "            f\"{custom_name}_units_layer_{i}\", min_units, max_units, step=unit_step\n",
    "        )\n",
    "\n",
    "        activation = get_activation(trial, f\"{custom_name}_activation_layer_{i}\")\n",
    "\n",
    "        kernel_reg = (\n",
    "            get_regularizer(trial, f\"{custom_name}_kernel_regularizer_{i}\")\n",
    "            if use_regularization\n",
    "            else None\n",
    "        )\n",
    "        bias_reg = (\n",
    "            get_regularizer(trial, f\"{custom_name}_bias_regularizer_{i}\")\n",
    "            if use_regularization\n",
    "            else None\n",
    "        )\n",
    "        activity_reg = (\n",
    "            get_regularizer(trial, f\"{custom_name}_activity_regularizer_{i}\")\n",
    "            if use_regularization\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        x = layers.LSTM(\n",
    "            units=units,\n",
    "            activation=activation,\n",
    "            return_sequences=(i < max_layers - 1),\n",
    "            recurrent_dropout=trial.suggest_float(\n",
    "                f\"lstm_recurrent_dropout_layer_{i}\", 0.0, 0.5, step=0.1\n",
    "            ),\n",
    "            dropout=trial.suggest_float(\n",
    "                f\"lstm_dropout_layer_{i}\", 0.0, 0.5, step=0.1\n",
    "            ),\n",
    "            kernel_regularizer=kernel_reg,\n",
    "            bias_regularizer=bias_reg,\n",
    "            activity_regularizer=activity_reg,\n",
    "            name=f\"{custom_name}_lstm_{i}\",\n",
    "        )(x)\n",
    "\n",
    "        if try_batch_norm and trial.suggest_categorical(\n",
    "            f\"{custom_name}_use_batch_norm_{i}\", [True, False]\n",
    "        ):\n",
    "            x = layers.BatchNormalization(name=f\"{custom_name}_batch_norm_{i}\")(x)\n",
    "\n",
    "        # ————————————————————————— Residual Connection Logic ———————————————————————— #\n",
    "        # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        #! EXPERIMENTAL FEATURE\n",
    "        #! UNTESTED, UNSURE IF IT WORKS\n",
    "        if residual_method == \"beside\":\n",
    "            if i == 0:\n",
    "                residual_tensor = x\n",
    "            else:\n",
    "                if trial.suggest_categorical(\n",
    "                    f\"{custom_name}_use_residual_{i}\", [True, False]\n",
    "                ):\n",
    "                    if residual_tensor.shape[-1] != x.shape[-1]:\n",
    "                        residual_tensor = layers.Dense(\n",
    "                            x.shape[-1],\n",
    "                            activation=None,\n",
    "                            name=f\"{custom_name}_res_dense_{i}\",\n",
    "                        )(residual_tensor)\n",
    "                    x = layers.Add(name=f\"{custom_name}_add_{i}\")([x, residual_tensor])\n",
    "                    residual_tensor = x\n",
    "                else:\n",
    "                    residual_tensor = x\n",
    "\n",
    "        elif residual_method == \"all\":\n",
    "            if i == 0:\n",
    "                skip_tensors = [x]\n",
    "            else:\n",
    "                additions = []\n",
    "                for j, prev in enumerate(skip_tensors):\n",
    "                    if trial.suggest_categorical(\n",
    "                        f\"{custom_name}_use_skip_{i}_{j}\", [True, False]\n",
    "                    ):\n",
    "                        adj = prev\n",
    "                        if adj.shape[-1] != x.shape[-1]:\n",
    "                            adj = layers.Dense(\n",
    "                                x.shape[-1],\n",
    "                                activation=None,\n",
    "                                name=f\"{custom_name}_skip_dense_{i}_{j}\",\n",
    "                            )(adj)\n",
    "                        additions.append(adj)\n",
    "                if additions:\n",
    "                    x = layers.Add(name=f\"{custom_name}_add_all_{i}\")([x] + additions)\n",
    "                skip_tensors.append(x)\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Objective Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(\n",
    "    trial: optuna.Trial,\n",
    "    X: List[np.ndarray],\n",
    "    y: List[np.ndarray],\n",
    "    checkpoint_dir: str,\n",
    "    model_dir: str,\n",
    "    fig_dir: str,\n",
    "    logs_dir: str,\n",
    "    epochs: int = 50,\n",
    "    size_penalizer: Optional[str] = None,\n",
    "    use_regularization: bool = False,\n",
    "    residual_method: Optional[str] = None,\n",
    "    show_summary: bool = False,\n",
    "    plot_model: bool = False,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Objective function for Optuna to optimize a Neural Network NN on any-input data.\n",
    "\n",
    "    Args:\n",
    "        trial (optuna.Trial): Current trial for hyperparameter suggestions.\n",
    "        X (List[np.ndarray]): List of input arrays.\n",
    "        y (List[np.ndarray]): List of label arrays.\n",
    "        checkpoint_dir (str): Path to store checkpoint files.\n",
    "        model_dir (str): Path to store full models.\n",
    "        fig_dir (str): Path to store plots.\n",
    "        logs_dir (str): Path to store logs.\n",
    "        epochs (int): Number of training epochs.\n",
    "        size_penalizer (Optional[str]): type of penalizer to use:\n",
    "            - \"params\": Penalizes based on the number of parameters.\n",
    "            - \"flops\": Penalizes based on the number of FLOPs.\n",
    "            - None: No penalization is applied.\n",
    "        use_regularization (bool): If True, adds regularization (e.g., L1/L2) to layers to prevent overfitting.\n",
    "        residual_method (Optional[str]): tyoe of residual connection to use:\n",
    "            - \"beside\": Adds residual connections between consecutive layers.\n",
    "            - \"all\": test residual connections between all layers.\n",
    "            - None: No residual connections are applied.\n",
    "        show_summary (bool): If True, display the model summary.\n",
    "        plot_model (bool): If True, display a plot of the model architecture.\n",
    "\n",
    "    Returns:\n",
    "        float: Final validation loss (optionally penalized) used for optimization.\n",
    "    \"\"\"\n",
    "\n",
    "    # ————————————————————————————— Prepare the Data ————————————————————————————— #\n",
    "    X_train, X_val = X[0], X[1]\n",
    "    y_train, y_val = y[0], y[1]\n",
    "    \n",
    "    n_ports = X_train.shape[1]\n",
    "    \n",
    "    # ———————————————————————————————————————————————————————————————————————————— #\n",
    "\n",
    "    model = None\n",
    "    try:\n",
    "\n",
    "        # ———————————————————————————————————————————————————————————————————————————— #\n",
    "        #                              Model Construction                              #\n",
    "        # ———————————————————————————————————————————————————————————————————————————— #\n",
    "\n",
    "        # —————————————————————————————————— Scaler —————————————————————————————————— #\n",
    "        scaler = get_scaler(trial)\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_val = scaler.transform(X_val)\n",
    "        \n",
    "        # ————————————————————————————— Reshape for lstm ————————————————————————————— #\n",
    "        # — Reshape for LSTM input: (batch, features) → (batch, 1, features)\n",
    "        X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "        X_val = X_val.reshape(X_val.shape[0], 1, X_val.shape[1])\n",
    "\n",
    "        # ——————————————————————————— Observed ports input ——————————————————————————— #\n",
    "        # Time-series data with sequence length 1 (batch_size, 1, observed_ports)\n",
    "        inputs = layers.Input(shape=(1, n_ports))  # Observed ports as input\n",
    "\n",
    "        max_layers = trial.suggest_int(\"num_layers\", 1, 6)\n",
    "\n",
    "        x = build_dynamic_lstm(\n",
    "            trial,\n",
    "            inputs,\n",
    "            max_layers=max_layers,\n",
    "            max_units=256,\n",
    "            min_units=32,\n",
    "            unit_step=32,\n",
    "            try_batch_norm=False,\n",
    "            use_regularization=use_regularization,\n",
    "            residual_method=residual_method,\n",
    "        )\n",
    "        \n",
    "        # ——————————————————————————————— Dense Layers ——————————————————————————————— #\n",
    "        #? This was the best performing option in the previous trials\n",
    "        num_dense_layers = trial.suggest_int(\"num_dense_layers\", 0, 3)\n",
    "        for i in range(num_dense_layers):\n",
    "            # Suggest the number of units for each dense layer\n",
    "            units = trial.suggest_int(f\"dense_{i+1}_units\", 64, 512, step=64)\n",
    "            x = layers.Dense(\n",
    "                units=units,\n",
    "                activation=get_activation(trial, f\"dense_{i+1}_activation\"),\n",
    "                name=f\"dense_{i+1}\",\n",
    "            )(x)\n",
    "            rate = trial.suggest_float(f\"dense_{i+1}_dropout\", 0.0, 0.5, step=0.1)\n",
    "            x = layers.Dropout(rate=rate)(x)\n",
    "\n",
    "        # —————————————————————————————————— Output —————————————————————————————————— #\n",
    "        outputs = layers.Dense(TOTAL_NUM_PORTS, activation=\"linear\")(x)\n",
    "\n",
    "        # —————————————————————————— Set Inputs and Outputs —————————————————————————— #\n",
    "        model = Model(inputs=inputs, outputs=(outputs,))\n",
    "\n",
    "        # ———————————————————————————— Vizualize the Model ——————————————————————————— #\n",
    "        if show_summary:\n",
    "            model.summary()\n",
    "\n",
    "        if plot_model:\n",
    "            # Display the model architecture image\n",
    "            tf.keras.utils.plot_model(\n",
    "                model,\n",
    "                to_file=os.path.join(fig_dir, f\"model_plot_{trial.number}.png\"),\n",
    "                show_shapes=True,\n",
    "                show_layer_names=True,\n",
    "            )\n",
    "            display(Image(filename=os.path.join(fig_dir, f\"model_plot_{trial.number}.png\")))\n",
    "\n",
    "        # ————————————————————————————— Compile the Model ———————————————————————————— #\n",
    "        optimizer = get_optimizer(trial)\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=\"mse\",\n",
    "        )\n",
    "\n",
    "        # ———————————————————————————————— Train Model ——————————————————————————————— #\n",
    "        batch_size = trial.suggest_categorical(\"batch_size\", [64, 128, 256])\n",
    "        history = model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=get_callbacks(trial, checkpoint_dir),\n",
    "            verbose=2,\n",
    "        )\n",
    "\n",
    "        model.save(os.path.join(model_dir, f\"trial_{trial.number}.keras\"))\n",
    "        loss = min(history.history[\"val_loss\"])\n",
    "\n",
    "        # ———————————————————————————————————————————————————————————————————————————— #\n",
    "        #                                 Trial Results                                #\n",
    "        # ———————————————————————————————————————————————————————————————————————————— #\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        epochs = list(range(1, len(history.history[\"loss\"]) + 1))\n",
    "        train_loss = history.history[\"loss\"]\n",
    "        val_loss = history.history[\"val_loss\"]\n",
    "    \n",
    "        # Create figure\n",
    "        fig, ax_loss = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "        # Plot Loss\n",
    "        ax_loss.plot(epochs, train_loss, marker=\"o\", linestyle=\"-\", label=\"Training Loss\")\n",
    "        ax_loss.plot(epochs, val_loss, marker=\"x\", linestyle=\"--\", label=\"Validation Loss\")\n",
    "        ax_loss.set_title(\"Training & Validation Loss\")\n",
    "        ax_loss.set_xlabel(\"Epoch\")\n",
    "        ax_loss.set_ylabel(\"Loss\")\n",
    "        ax_loss.set_xticks(epochs)\n",
    "        ax_loss.set_ylim(0, max(max(train_loss), max(val_loss)) * 1.05)\n",
    "        ax_loss.grid(True)\n",
    "        ax_loss.legend()\n",
    "\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(os.path.join(fig_dir, f\"trial_{trial.number}.png\"), dpi=300)\n",
    "        plt.close(fig)\n",
    "\n",
    "        # ————————————————————————————————— Evaluate ————————————————————————————————— #\n",
    "        datasets = [\n",
    "            kappa0_mu1_m0_test,\n",
    "            kappa0_mu1_m2_test,\n",
    "            kappa0_mu1_m50_test,\n",
    "            kappa0_mu2_m50_test,\n",
    "            kappa0_mu5_m50_test,\n",
    "            kappa5_mu1_m0_test,\n",
    "            kappa5_mu1_m2_test,\n",
    "            kappa5_mu1_m50_test,\n",
    "            kappa5_mu2_m0_test,\n",
    "            kappa5_mu2_m2_test,\n",
    "            kappa5_mu2_m50_test,\n",
    "            kappa5_mu5_m0_test,\n",
    "            kappa5_mu5_m2_test,\n",
    "            kappa5_mu5_m50_test,\n",
    "        ]\n",
    "        test_losses = []\n",
    "        ops = []\n",
    "\n",
    "        for i, dataset in enumerate(datasets, start=1):\n",
    "            observed_ports, observed_indices = get_observed_ports(\n",
    "                dataset, num_observed_ports=n_ports, total_ports=TOTAL_NUM_PORTS\n",
    "            )\n",
    "            \n",
    "            X_test, y_test = observed_ports, dataset\n",
    "            X_test = scaler.transform(X_test)\n",
    "            \n",
    "            # Reshape for LSTM\n",
    "            X_test = X_test.reshape(-1, 1, X_test.shape[1])\n",
    "            \n",
    "            test_loss = model.evaluate(X_test, y_test, batch_size=batch_size, verbose=0)\n",
    "            test_losses.append(test_loss)\n",
    "            \n",
    "            # Make predictions\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "            # Calculate Outage Probability (OP)\n",
    "            op_value = getOP(\n",
    "                observed_indices=observed_indices,\n",
    "                predicted_values=y_pred,\n",
    "                true_values=y_test,\n",
    "                threshold=THRESHOLD,\n",
    "                snr_linear=SNR_LINEAR,\n",
    "                total_ports=TOTAL_NUM_PORTS,\n",
    "            )\n",
    "            ops.append(op_value)\n",
    "\n",
    "        dataset_names: list[str] = [\n",
    "            \"kappa0_mu1_m0_test\",\n",
    "            \"kappa0_mu1_m2_test\",\n",
    "            \"kappa0_mu1_m50_test\",\n",
    "            \"kappa0_mu2_m50_test\",\n",
    "            \"kappa0_mu5_m50_test\",\n",
    "            \"kappa5_mu1_m0_test\",\n",
    "            \"kappa5_mu1_m2_test\",\n",
    "            \"kappa5_mu1_m50_test\",\n",
    "            \"kappa5_mu2_m0_test\",\n",
    "            \"kappa5_mu2_m2_test\",\n",
    "            \"kappa5_mu2_m50_test\",\n",
    "            \"kappa5_mu5_m0_test\",\n",
    "            \"kappa5_mu5_m2_test\",\n",
    "            \"kappa5_mu5_m50_test\",\n",
    "        ]\n",
    "\n",
    "        dataset_test_losses = {\n",
    "            f\"{name}_loss\": loss\n",
    "            for name, loss in zip(dataset_names, test_losses)\n",
    "        }\n",
    "        dataset_test_ops = {\n",
    "            f\"{name}_op\": op\n",
    "            for name, op in zip(dataset_names, ops)\n",
    "        }\n",
    "\n",
    "        # Print test losses in a formatted manner\n",
    "        print(\"\\n\" + \"=\" * 15)\n",
    "        for name in dataset_names:\n",
    "            loss = dataset_test_losses[f\"{name}_loss\"]\n",
    "            op   = dataset_test_ops[f\"{name}_op\"]\n",
    "            print(\n",
    "                f\"{name.replace('_', ' ').capitalize()}: \"\n",
    "                f\"Loss = {loss:.12f}, OP = {op:.12f}\"\n",
    "            )\n",
    "\n",
    "        params = model.count_params()\n",
    "        print(f\"\\nNumber of parameters: {params}\")\n",
    "        print(f\"Model size: {params * 4 / (1024 ** 2):.2f} MB\")\n",
    "        print(\"=\" * 15 + \"\\n\")\n",
    "        \n",
    "        trial.set_user_attr(\"num_params\", params)\n",
    "        trial.set_user_attr(\"model_size_mb\", params * 4 / (1024 ** 2))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    except optuna.exceptions.TrialPruned:\n",
    "        raise  # simply propagate pruning\n",
    "    except tf.errors.ResourceExhaustedError as oom_err:\n",
    "        # Catch OOM / resource exhausted\n",
    "        print(f\"❌ Trial {trial.number} hit OOM (ResourceExhaustedError): {oom_err}\")\n",
    "\n",
    "        # Log the error to a file in the logs directory\n",
    "        error_log_path = os.path.join(logs_dir, f\"trial_{trial.number}_error.log\")\n",
    "        with open(error_log_path, \"w\") as log_file:\n",
    "            log_file.write(f\"Trial {trial.number} encountered an error:\\n\")\n",
    "            log_file.write(str(oom_err) + \"\\n\\n\")\n",
    "            log_file.write(\"Traceback:\\n\")\n",
    "            traceback.print_exc(file=log_file)\n",
    "\n",
    "        return float(\"inf\")  # Return bad loss\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during the trial execution: {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "        # Log the error to a file in the logs directory\n",
    "        error_log_path = os.path.join(logs_dir, f\"trial_{trial.number}_error.log\")\n",
    "        with open(error_log_path, \"w\") as log_file:\n",
    "            log_file.write(f\"Trial {trial.number} encountered an error:\\n\")\n",
    "            log_file.write(str(e) + \"\\n\\n\")\n",
    "            log_file.write(\"Traceback:\\n\")\n",
    "            traceback.print_exc(file=log_file)\n",
    "\n",
    "        return float(\"inf\")  # Return bad loss\n",
    "    finally:\n",
    "        if model is not None:\n",
    "            clear_session()\n",
    "            del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Code Health Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resources_dir = os.path.join(RUN_DIR, \"resources\")\n",
    "# os.makedirs(resources_dir, exist_ok=True)\n",
    "# troo.log_resources(log_dir=resources_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    pid = os.getpid()\n",
    "    cmd = (\n",
    "        f'python3 \"{os.path.abspath(\"_monitor_kernel_life.py\")}\" '\n",
    "        f\"--pid {pid} --custom-title {RUN_DIR}; exec bash\"\n",
    "    )\n",
    "    terminals = [\n",
    "        [\"xfce4-terminal\", \"--disable-server\", \"--hold\", \"-e\", f'bash -c \"{cmd}\"'],\n",
    "        [\"gnome-terminal\", \"--disable-factory\", \"--\", \"bash\", \"-i\", \"-c\", cmd],\n",
    "        [\"xterm\", \"-hold\", \"-e\", cmd],\n",
    "        [\"konsole\", \"--hold\", \"-e\", f'bash -c \"{cmd}\"'],\n",
    "    ]\n",
    "    term = next((t for t in terminals if shutil.which(t[0])), None)\n",
    "    if not term:\n",
    "        raise RuntimeError(\n",
    "            \"No supported terminal emulator found; install gnome-terminal, \"\n",
    "            \"xfce4-terminal, konsole, or xterm.\"\n",
    "        )\n",
    "    _monitor_proc = subprocess.Popen(term, preexec_fn=os.setpgrp)\n",
    "    print(f\"[INFO] Launched monitor in {term[0]} (PID={pid})\")\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Auto launching kernel monitoring failed! {e}\\n\")\n",
    "    display(\n",
    "        HTML(\n",
    "            f\"Call the monitor script manually: \"\n",
    "            f'<span style=\"color: orange;\">'\n",
    "            f\"python _monitor_kernel_life.py --pid {pid} --custom-title {RUN_DIR}\"\n",
    "            f\"</span>\"\n",
    "        )\n",
    "    )\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in observed_ports_list:\n",
    "    try:\n",
    "        # ——————————————————————————————— Storage paths —————————————————————————————— #\n",
    "        study_dir = os.path.join(RUN_DIR, f\"optuna_study_{n}_ports\")\n",
    "        os.makedirs(study_dir, exist_ok=True)\n",
    "\n",
    "        dirs = {\n",
    "            \"args\": os.path.join(study_dir, \"args\"),\n",
    "            \"figures\": os.path.join(study_dir, \"figures\"),\n",
    "            \"weights\": os.path.join(study_dir, \"weights\"),\n",
    "            \"models\": os.path.join(study_dir, \"models\"),\n",
    "            \"logs\": os.path.join(study_dir, \"logs\"),\n",
    "        }\n",
    "        for path in dirs.values():\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "\n",
    "        storage_path = f\"sqlite:///{os.path.join(study_dir, 'optuna_study.db')}\"\n",
    "        checkpoint_dir, model_dir, fig_dir, args_dir, logs_dir = (\n",
    "            dirs[\"weights\"],\n",
    "            dirs[\"models\"],\n",
    "            dirs[\"figures\"],\n",
    "            dirs[\"args\"],\n",
    "            dirs[\"logs\"],\n",
    "        )\n",
    "\n",
    "        print(f\"Initializing study at '{study_dir}'...\")\n",
    "        \n",
    "        # ——————————————————————————————————— Data ——————————————————————————————————— #\n",
    "        observed_ports, observed_indices = get_observed_ports(\n",
    "            dataset, num_observed_ports=n, total_ports=TOTAL_NUM_PORTS\n",
    "        )\n",
    "        \n",
    "        # split the dataset into training and validation sets\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            observed_ports,\n",
    "            dataset,\n",
    "            test_size=0.2,\n",
    "            random_state=0,\n",
    "            shuffle=True,\n",
    "        )\n",
    "            \n",
    "        # —————————————————————————————————— Pruners ————————————————————————————————— #\n",
    "        pruner = optuna.pruners.HyperbandPruner()\n",
    "\n",
    "        # ——————————————————————————————————— Study —————————————————————————————————— #\n",
    "        study = optuna.create_study(\n",
    "            study_name=os.path.basename(study_dir),\n",
    "            storage=storage_path,\n",
    "            direction=\"minimize\",\n",
    "            pruner=pruner,\n",
    "            load_if_exists=True,\n",
    "        )\n",
    "\n",
    "        # Count trials done, then determine the remaining trials\n",
    "        done_trials = len(\n",
    "            study.get_trials(\n",
    "                deepcopy=False,\n",
    "                states=(\n",
    "                    optuna.trial.TrialState.COMPLETE,\n",
    "                    optuna.trial.TrialState.PRUNED,\n",
    "                    optuna.trial.TrialState.FAIL,\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "        n_remaining_trials = max(0, NUM_TRIALS - done_trials)\n",
    "\n",
    "        study.optimize(\n",
    "            lambda trial: objective(\n",
    "                trial,\n",
    "                X=[X_train, X_val],\n",
    "                y=[y_train, y_val],\n",
    "                checkpoint_dir=checkpoint_dir,\n",
    "                model_dir=model_dir,\n",
    "                fig_dir=fig_dir,\n",
    "                logs_dir=logs_dir,\n",
    "                epochs=EPOCHS,\n",
    "                size_penalizer=None,\n",
    "                use_regularization=False,\n",
    "                residual_method=None,  #! Find your backbone first\n",
    "                show_summary=False,\n",
    "            ),\n",
    "            n_trials=n_remaining_trials,\n",
    "            catch=(ValueError, RuntimeError),\n",
    "            gc_after_trial=True,\n",
    "            n_jobs=1,  # If you have multiple GPUs/Cores\n",
    "            show_progress_bar=False,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kill the monitor kernel life process\n",
    "if _monitor_proc is not None and _monitor_proc.poll() is None:\n",
    "    os.killpg(_monitor_proc.pid, signal.SIGINT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-optuna",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
